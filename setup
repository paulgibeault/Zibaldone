#!/bin/bash
set -e

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== Zibaldone Setup ===${NC}"

# 1. Environment Checks
# Function to check and optionally install dependencies
check_and_install() {
    local cmd=$1
    local package=$2
    local pretty_name=$3

    if ! command -v "$cmd" &> /dev/null; then
        echo -e "${RED}Error: $pretty_name is not installed.${NC}"
        
        if command -v brew &> /dev/null; then
            read -p "  Would you like to try installing it via Homebrew? [y/N] " -n 1 -r
            echo
            if [[ $REPLY =~ ^[Yy]$ ]]; then
                echo -e "${BLUE}  Installing $package...${NC}"
                if brew install "$package"; then
                    echo -e "${GREEN}  ✓ $pretty_name installed successfully.${NC}"
                    return 0
                else
                    echo -e "${RED}  Failed to install $package.${NC}"
                    exit 1
                fi
            else
                echo -e "${RED}  Please install $pretty_name manually to continue.${NC}"
                exit 1
            fi
        else
            echo -e "${RED}  Homebrew not found. Please install $pretty_name manually.${NC}"
            # Provide specific URLs for common tools
            case $cmd in
                node) echo -e "  Download at: https://nodejs.org/" ;;
                python3) echo -e "  Download at: https://www.python.org/downloads/" ;;
            esac
            exit 1
        fi
    fi
}

echo -e "\n${BLUE}[1/6] Checking System Dependencies...${NC}"

check_and_install "python3" "python" "Python 3"
check_and_install "node" "node" "Node.js"
# npm usually comes with node, but good to check just in case
check_and_install "npm" "node" "npm"

echo -e "${GREEN}✓ System dependencies found.${NC}"

# 2. Interactive Configuration
echo -e "\n${BLUE}[2/6] Configuration Setup...${NC}"
echo "Please enter your LLM Configuration (Press enter for defaults)"

# Ask for Model ID
DEFAULT_MODEL="openai/gpt-oss-20b"
read -p "LM Studio Model ID [${DEFAULT_MODEL}]: " USER_MODEL
USER_MODEL=${USER_MODEL:-$DEFAULT_MODEL}

# Ask for Base URL
DEFAULT_URL="http://localhost:1234/v1"
read -p "LM Studio Base URL [${DEFAULT_URL}]: " USER_URL
USER_URL=${USER_URL:-$DEFAULT_URL}

echo -e "Using Model: ${GREEN}${USER_MODEL}${NC}"
echo -e "Using URL:   ${GREEN}${USER_URL}${NC}"

# 3. Backend Setup
echo -e "\n${BLUE}[3/6] Setting up Backend...${NC}"
cd backend

if [ ! -d ".venv" ]; then
    echo "Creating virtual environment..."
    python3 -m venv .venv
fi

source .venv/bin/activate

echo "Installing Python dependencies..."
pip install -U pip > /dev/null
pip install -r requirements.txt
pip install 'litellm[proxy]'

# Create .env
# Always recreate or update? For now, we'll only create if missing to avoid overwriting secrets,
# BUT since we just asked for config, the user probably expects it to apply.
# Let's simple check if file exists and warn, or just overwrite?
# The safest interactive pattern is likely "If exists, ask to overwrite".
# For simplicity in this v1 script, we'll backup existing .env if it exists and write new one.

if [ -f ".env" ]; then
    echo "Backing up existing .env to .env.bak"
    cp .env .env.bak
fi

echo "Generating .env file..."
# CRITICAL FIX: The LLM_MODEL must allow litellm to know the provider. 
# We mapped the user's model to 'zibaldone-model' in litellm config (step 5).
# Here we tell backend to use 'openai/zibaldone-model' so it treats the proxy as OpenAI compatible.
cat > .env <<EOL
# Zibaldone Backend Configuration
LLM_MODEL=openai/zibaldone-model
OPENAI_API_BASE=http://localhost:4000
OPENAI_API_KEY=sk-any
EOL
echo -e "${GREEN}✓ Created backend/.env${NC}"

cd ..

# 4. Frontend Setup
echo -e "\n${BLUE}[4/6] Setting up Frontend...${NC}"
cd frontend
if [ ! -d "node_modules" ]; then
    echo "Installing Node modules..."
    npm install
else
    echo "Node modules already installed."
fi
cd ..

# 5. Global Configuration
echo -e "\n${BLUE}[5/6] Generating Proxy Configuration...${NC}"

# Generate litellm_config.yaml
# We map the abstract name "zibaldone-model" to the specific user provided model/url.
cat > litellm_config.yaml <<EOL
model_list:
  # Configured via setup script
  - model_name: zibaldone-model
    litellm_params:
      model: ${USER_MODEL}
      api_base: "${USER_URL}"
      api_key: "any-string"

  # Fallback
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
EOL
echo -e "${GREEN}✓ Created litellm_config.yaml${NC}"

# 6. Readiness Check
echo -e "\n${BLUE}[6/6] Checking Readiness...${NC}"
source backend/.venv/bin/activate

if command -v uvicorn &> /dev/null && command -v litellm &> /dev/null; then
     echo -e "${GREEN}✓ Backend tools (uvicorn, litellm) are ready.${NC}"
else
     echo -e "${RED}Error: Backend tools not found in venv.${NC}"
     exit 1
fi

echo -e "\n${GREEN}=== Setup Complete! ===${NC}"
echo "You can now run: ./go"
