#!/bin/bash
set -e

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== Zibaldone Setup ===${NC}"

# 1. Environment Checks
echo -e "\n${BLUE}[1/6] Checking System Dependencies...${NC}"
if ! command -v python3 &> /dev/null; then
    echo -e "${RED}Error: python3 is not installed.${NC}"
    exit 1
fi
if ! command -v node &> /dev/null; then
    echo -e "${RED}Error: node is not installed.${NC}"
    exit 1
fi
if ! command -v npm &> /dev/null; then
    echo -e "${RED}Error: npm is not installed.${NC}"
    exit 1
fi
echo -e "${GREEN}✓ System dependencies found.${NC}"

# 2. Interactive Configuration
echo -e "\n${BLUE}[2/6] Configuration Setup...${NC}"
echo "Please enter your LLM Configuration (Press enter for defaults)"

# Ask for Model ID
DEFAULT_MODEL="openai/gpt-oss-20b"
read -p "LM Studio Model ID [${DEFAULT_MODEL}]: " USER_MODEL
USER_MODEL=${USER_MODEL:-$DEFAULT_MODEL}

# Ask for Base URL
DEFAULT_URL="http://localhost:1234/v1"
read -p "LM Studio Base URL [${DEFAULT_URL}]: " USER_URL
USER_URL=${USER_URL:-$DEFAULT_URL}

echo -e "Using Model: ${GREEN}${USER_MODEL}${NC}"
echo -e "Using URL:   ${GREEN}${USER_URL}${NC}"

# 3. Backend Setup
echo -e "\n${BLUE}[3/6] Setting up Backend...${NC}"
cd backend

if [ ! -d ".venv" ]; then
    echo "Creating virtual environment..."
    python3 -m venv .venv
fi

source .venv/bin/activate

echo "Installing Python dependencies..."
pip install -U pip > /dev/null
pip install -r requirements.txt
pip install 'litellm[proxy]'

# Create .env
# Always recreate or update? For now, we'll only create if missing to avoid overwriting secrets,
# BUT since we just asked for config, the user probably expects it to apply.
# Let's simple check if file exists and warn, or just overwrite?
# The safest interactive pattern is likely "If exists, ask to overwrite".
# For simplicity in this v1 script, we'll backup existing .env if it exists and write new one.

if [ -f ".env" ]; then
    echo "Backing up existing .env to .env.bak"
    cp .env .env.bak
fi

echo "Generating .env file..."
# CRITICAL FIX: The LLM_MODEL must allow litellm to know the provider. 
# We mapped the user's model to 'zibaldone-model' in litellm config (step 5).
# Here we tell backend to use 'openai/zibaldone-model' so it treats the proxy as OpenAI compatible.
cat > .env <<EOL
# Zibaldone Backend Configuration
LLM_MODEL=openai/zibaldone-model
OPENAI_API_BASE=http://localhost:4000
OPENAI_API_KEY=sk-any
EOL
echo -e "${GREEN}✓ Created backend/.env${NC}"

cd ..

# 4. Frontend Setup
echo -e "\n${BLUE}[4/6] Setting up Frontend...${NC}"
cd frontend
if [ ! -d "node_modules" ]; then
    echo "Installing Node modules..."
    npm install
else
    echo "Node modules already installed."
fi
cd ..

# 5. Global Configuration
echo -e "\n${BLUE}[5/6] Generating Proxy Configuration...${NC}"

# Generate litellm_config.yaml
# We map the abstract name "zibaldone-model" to the specific user provided model/url.
cat > litellm_config.yaml <<EOL
model_list:
  # Configured via setup script
  - model_name: zibaldone-model
    litellm_params:
      model: ${USER_MODEL}
      api_base: "${USER_URL}"
      api_key: "any-string"

  # Fallback
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
EOL
echo -e "${GREEN}✓ Created litellm_config.yaml${NC}"

# 6. Readiness Check
echo -e "\n${BLUE}[6/6] Checking Readiness...${NC}"
source backend/.venv/bin/activate

if command -v uvicorn &> /dev/null && command -v litellm &> /dev/null; then
     echo -e "${GREEN}✓ Backend tools (uvicorn, litellm) are ready.${NC}"
else
     echo -e "${RED}Error: Backend tools not found in venv.${NC}"
     exit 1
fi

echo -e "\n${GREEN}=== Setup Complete! ===${NC}"
echo "You can now run: ./go"
